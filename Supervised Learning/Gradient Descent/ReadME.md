# The Gradient Descent Algorithm
![](https://d2o2utebsixu4k.cloudfront.net/media/images/1566457804835-Image-18.jpg)

## What is Gradient Descent?

Gradient descent is an optimization algorithm which is mainly used to find the minimum of a function. It can be used with other machine learning algorithms.


## How does Gradient Descent work?

The objective in the case of gradient descent is to find a line of best fit for some given features (inputs), or X values, and any number of labels (outputs) or  Y values. 
A cost function is defined as: a function that maps an event or values of one or more variables onto a real number intuitively representing some “cost” associated with the event.

With a known set of inputs and their corresponding outputs, a machine learning model attempts to make predictions according to the new set of inputs.

## Minimizing the Cost Function
It is always the primary goal of any Machine Learning Algorithm to minimize the Cost Function. 
Minimizing cost functions will also result in a lower error between the predicted values and the actual values which also denotes that the algorithm has performed well in learning. 

 In order to minimize the function, we  use the Gradient Descent algorithm to locate the minima. The most common function which is often used is the  **mean squared error**. It measures the difference between the estimated value (the prediction) and the estimator (the dataset).
 ![](https://d2o2utebsixu4k.cloudfront.net/media/images/1566459128498-Image-33.jpg)
